{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1 = pd.read_csv(\"fold1.csv\") \n",
    "fold2 = pd.read_csv(\"fold2.csv\") \n",
    "fold3 = pd.read_csv(\"fold3.csv\") \n",
    "fold4 = pd.read_csv(\"fold4.csv\") \n",
    "fold5 = pd.read_csv(\"fold5.csv\") \n",
    "\n",
    "whole_df = pd.concat([fold1,fold2,fold3,fold4,fold5], ignore_index=True)\n",
    "lowercased = 0\n",
    "contractioned = 0\n",
    "no_punctuation = 0\n",
    "no_stopwords = 0\n",
    "no_frequent_words = 0\n",
    "lemmatization = 0\n",
    "stemming = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOWERCASE\n",
    "def custom_lowercase(text):\n",
    "    \"\"\"custom function to lowercase the words that only their first character is capital\"\"\"\n",
    "    new_text = \"\"\n",
    "    for word in str(text).split():\n",
    "        if len(word)>1:\n",
    "            if word[0].isupper() and word[1].islower():\n",
    "                word = word[0].lower() + word[1:]\n",
    "                new_text = new_text +\" \"+ word\n",
    "            else:\n",
    "                new_text = new_text +\" \"+ word\n",
    "        elif word[0].isupper():\n",
    "                word = word[0].lower() \n",
    "                new_text = new_text +\" \"+ word\n",
    "        else:\n",
    "            new_text = new_text +\" \"+ word\n",
    "    return new_text\n",
    "\n",
    "\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: custom_lowercase(text))\n",
    "lowercased = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTRACTIONS\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda x: contractions.fix(x))\n",
    "contractioned = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE PUNCTUATION\n",
    "#the \"0\" index of the list is \"!\" so we skip it\n",
    "punctuation_list = string.punctuation[1:]\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', punctuation_list))\n",
    "\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: remove_punctuation(text))\n",
    "no_punctuation =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE STOPWORDS\n",
    "#nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: remove_stopwords(text))\n",
    "no_stopwords = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE FREQUENT WORDS\n",
    "#seperate the dataframes\n",
    "fake_df = whole_df.loc[whole_df['Fake'] == 1]\n",
    "real_df = whole_df.loc[whole_df['Fake'] == 0]\n",
    "\n",
    "def counts(df):\n",
    "    cnt = Counter()\n",
    "    for text in df[\"Review\"].values:\n",
    "        for word in text.split():\n",
    "            cnt[word] += 1\n",
    "    return cnt\n",
    "\n",
    "fake_cnts = counts(fake_df)\n",
    "real_cnts = counts(real_df)\n",
    "        \n",
    "def get_list(lis):\n",
    "    words = []\n",
    "    for pair in lis:\n",
    "        words.append(pair[0])\n",
    "    return words\n",
    "\n",
    "fake_common = get_list(fake_cnts.most_common(20))\n",
    "real_common = get_list(real_cnts.most_common(20))\n",
    "freq_to_remove = list(set(fake_common).intersection(real_common))\n",
    "\n",
    "def remove_freq(text, freq):\n",
    "    \"\"\"custom function to remove freq words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in freq])\n",
    "\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: remove_freq(text,freq_to_remove))\n",
    "no_frequent_words = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEMMATIZATION\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: lemmatize_words(text))\n",
    "lemmatization =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEMMING\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "#whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: stem_words(text))\n",
    "#stemming = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = whole_df['Review']\n",
    "Y = whole_df['Fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle = False)\n",
    "\n",
    "uni_vector = TfidfVectorizer(lowercase=False, ngram_range=(1, 1))\n",
    "features_uni = uni_vector.fit_transform(X_train).toarray()\n",
    "\n",
    "bi_vector = TfidfVectorizer(lowercase=False, ngram_range=(1, 2))\n",
    "features_bi = bi_vector.fit_transform(X_train).toarray()\n",
    "\n",
    "X_train_trans_uni = uni_vector.fit_transform(X_train)\n",
    "X_test_trans_uni = uni_vector.transform(X_test)\n",
    "\n",
    "X_train_trans_bi = bi_vector.fit_transform(X_train)\n",
    "X_test_trans_bi = bi_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "LOWERCASED\n",
      "----------\n",
      "CONTRACTIONED\n",
      "----------\n",
      "NO PUNCTUATION\n",
      "----------\n",
      "NO STOPWORDS\n",
      "----------\n",
      "NO FREQ WORDS\n",
      "----------\n",
      "LEMMATIZATION\n",
      "----------\n",
      "---------\n",
      "C= 0.01    P= l1\n",
      "Unigram model\n",
      "Accuracy score:  0.5\n",
      "Precision score:  0.0\n",
      "Recall score: 0.0\n",
      "F1 score: 0.0\n",
      "Confusion Matrix: [[80  0]\n",
      " [80  0]]\n",
      "\n",
      "Bigram model\n",
      "Accuracy score:  0.5\n",
      "Precision score:  0.0\n",
      "Recall score: 0.0\n",
      "F1 score: 0.0\n",
      "Confusion Matrix: [[80  0]\n",
      " [80  0]]\n",
      "---------\n",
      "C= 0.01    P= l2\n",
      "Unigram model\n",
      "Accuracy score:  0.85\n",
      "Precision score:  0.8181818181818182\n",
      "Recall score: 0.9\n",
      "F1 score: 0.8571428571428572\n",
      "Confusion Matrix: [[64 16]\n",
      " [ 8 72]]\n",
      "\n",
      "Bigram model\n",
      "Accuracy score:  0.85\n",
      "Precision score:  0.8043478260869565\n",
      "Recall score: 0.925\n",
      "F1 score: 0.8604651162790697\n",
      "Confusion Matrix: [[62 18]\n",
      " [ 6 74]]\n",
      "---------\n",
      "C= 0.1    P= l1\n",
      "Unigram model\n",
      "Accuracy score:  0.5\n",
      "Precision score:  0.0\n",
      "Recall score: 0.0\n",
      "F1 score: 0.0\n",
      "Confusion Matrix: [[80  0]\n",
      " [80  0]]\n",
      "\n",
      "Bigram model\n",
      "Accuracy score:  0.5\n",
      "Precision score:  0.0\n",
      "Recall score: 0.0\n",
      "F1 score: 0.0\n",
      "Confusion Matrix: [[80  0]\n",
      " [80  0]]\n",
      "---------\n",
      "C= 0.1    P= l2\n",
      "Unigram model\n",
      "Accuracy score:  0.8625\n",
      "Precision score:  0.881578947368421\n",
      "Recall score: 0.8375\n",
      "F1 score: 0.8589743589743589\n",
      "Confusion Matrix: [[71  9]\n",
      " [13 67]]\n",
      "\n",
      "Bigram model\n",
      "Accuracy score:  0.86875\n",
      "Precision score:  0.8554216867469879\n",
      "Recall score: 0.8875\n",
      "F1 score: 0.8711656441717791\n",
      "Confusion Matrix: [[68 12]\n",
      " [ 9 71]]\n",
      "---------\n",
      "C= 1    P= l1\n",
      "Unigram model\n",
      "Accuracy score:  0.775\n",
      "Precision score:  0.7972972972972973\n",
      "Recall score: 0.7375\n",
      "F1 score: 0.7662337662337663\n",
      "Confusion Matrix: [[65 15]\n",
      " [21 59]]\n",
      "\n",
      "Bigram model\n",
      "Accuracy score:  0.6625\n",
      "Precision score:  0.6710526315789473\n",
      "Recall score: 0.6375\n",
      "F1 score: 0.6538461538461537\n",
      "Confusion Matrix: [[55 25]\n",
      " [29 51]]\n",
      "---------\n",
      "C= 1    P= l2\n",
      "Unigram model\n",
      "Accuracy score:  0.8875\n",
      "Precision score:  0.918918918918919\n",
      "Recall score: 0.85\n",
      "F1 score: 0.8831168831168831\n",
      "Confusion Matrix: [[74  6]\n",
      " [12 68]]\n",
      "\n",
      "Bigram model\n",
      "Accuracy score:  0.875\n",
      "Precision score:  0.875\n",
      "Recall score: 0.875\n",
      "F1 score: 0.875\n",
      "Confusion Matrix: [[70 10]\n",
      " [10 70]]\n",
      "---------\n",
      "C= 10    P= l1\n",
      "Unigram model\n",
      "Accuracy score:  0.8125\n",
      "Precision score:  0.8472222222222222\n",
      "Recall score: 0.7625\n",
      "F1 score: 0.8026315789473685\n",
      "Confusion Matrix: [[69 11]\n",
      " [19 61]]\n",
      "\n",
      "Bigram model\n",
      "Accuracy score:  0.8\n",
      "Precision score:  0.8076923076923077\n",
      "Recall score: 0.7875\n",
      "F1 score: 0.7974683544303798\n",
      "Confusion Matrix: [[65 15]\n",
      " [17 63]]\n",
      "---------\n",
      "C= 10    P= l2\n",
      "Unigram model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giann\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.86875\n",
      "Precision score:  0.8933333333333333\n",
      "Recall score: 0.8375\n",
      "F1 score: 0.8645161290322579\n",
      "Confusion Matrix: [[72  8]\n",
      " [13 67]]\n",
      "\n",
      "Bigram model\n",
      "Accuracy score:  0.88125\n",
      "Precision score:  0.8860759493670886\n",
      "Recall score: 0.875\n",
      "F1 score: 0.8805031446540881\n",
      "Confusion Matrix: [[71  9]\n",
      " [10 70]]\n",
      "---------\n",
      "C= 100    P= l1\n",
      "Unigram model\n",
      "Accuracy score:  0.7875\n",
      "Precision score:  0.7948717948717948\n",
      "Recall score: 0.775\n",
      "F1 score: 0.7848101265822786\n",
      "Confusion Matrix: [[64 16]\n",
      " [18 62]]\n",
      "\n",
      "Bigram model\n",
      "Accuracy score:  0.80625\n",
      "Precision score:  0.8356164383561644\n",
      "Recall score: 0.7625\n",
      "F1 score: 0.7973856209150326\n",
      "Confusion Matrix: [[68 12]\n",
      " [19 61]]\n",
      "---------\n",
      "C= 100    P= l2\n",
      "Unigram model\n",
      "Accuracy score:  0.85625\n",
      "Precision score:  0.88\n",
      "Recall score: 0.825\n",
      "F1 score: 0.8516129032258064\n",
      "Confusion Matrix: [[71  9]\n",
      " [14 66]]\n",
      "\n",
      "Bigram model\n",
      "Accuracy score:  0.875\n",
      "Precision score:  0.875\n",
      "Recall score: 0.875\n",
      "F1 score: 0.875\n",
      "Confusion Matrix: [[70 10]\n",
      " [10 70]]\n",
      "BEST UNIGRAM has C= 1  and penalty= l2\n",
      "BEST BIGRAM has C= 10  and penalty= l2\n"
     ]
    }
   ],
   "source": [
    "#Building the RLR Model\n",
    "if (lowercased == 1):\n",
    "    print(\"----------\")\n",
    "    print(\"LOWERCASED\")\n",
    "if (contractioned ==1):\n",
    "    print(\"----------\")\n",
    "    print(\"CONTRACTIONED\")\n",
    "if (no_punctuation == 1):\n",
    "    print(\"----------\")\n",
    "    print(\"NO PUNCTUATION\")\n",
    "if (no_stopwords == 1):\n",
    "    print(\"----------\")\n",
    "    print(\"NO STOPWORDS\")\n",
    "if (no_frequent_words == 1):\n",
    "    print(\"----------\")\n",
    "    print(\"NO FREQ WORDS\")\n",
    "if (lemmatization == 1):\n",
    "    print(\"----------\")\n",
    "    print(\"LEMMATIZATION\")\n",
    "if (stemming == 1):\n",
    "    print(\"----------\")\n",
    "    print(\"STEMMING\")\n",
    "print(\"----------\")\n",
    "\n",
    "C_param_range = [0.01,0.1,1,10,100]\n",
    "Penalty = ['l1','l2']\n",
    "\n",
    "unimax = -1\n",
    "unibestC = -700\n",
    "unibestPen = 'none'\n",
    "\n",
    "bimax = -1\n",
    "bimax = -1\n",
    "bibestC = -700\n",
    "bibestPen = 'none'\n",
    "for c in C_param_range:\n",
    "    for p in Penalty:\n",
    "        print(\"---------\")\n",
    "        print(\"C=\",c,\"  \",\"P=\",p)    \n",
    "        #unigram\n",
    "        RLR_uni_model = LogisticRegression(solver='liblinear', penalty=p,C=c)\n",
    "        RLR_uni_model.fit(X_train_trans_uni, y_train)\n",
    "        predictions_uni = RLR_uni_model.predict(X_test_trans_uni)\n",
    "        \n",
    "        unimetrics = accuracy_score(y_test, predictions_uni) + precision_score(y_test, predictions_uni, zero_division=\"warn\") + recall_score(y_test, predictions_uni, zero_division=\"warn\") + f1_score(y_test, predictions_uni, zero_division=\"warn\")\n",
    "        \n",
    "        if(unimetrics>unimax):\n",
    "            unimax = unimetrics\n",
    "            unibestC = c\n",
    "            unibestPen = p         \n",
    "            \n",
    "        print(\"Unigram model\")\n",
    "        print(\"Accuracy score: \", accuracy_score(y_test, predictions_uni))\n",
    "        print(\"Precision score: \", precision_score(y_test, predictions_uni, zero_division=\"warn\"))\n",
    "        print(\"Recall score:\" , recall_score(y_test, predictions_uni, zero_division=\"warn\"))\n",
    "        print(\"F1 score:\" , f1_score(y_test, predictions_uni, zero_division=\"warn\"))\n",
    "        print(\"Confusion Matrix:\", confusion_matrix(y_test, predictions_uni))\n",
    "        print(\"\")\n",
    "\n",
    "        #bigram\n",
    "        RLR_uni_model = LogisticRegression(solver='liblinear', penalty=p,C=c)\n",
    "        RLR_uni_model.fit(X_train_trans_bi, y_train)\n",
    "        predictions_bi = RLR_uni_model.predict(X_test_trans_bi)\n",
    "        bimetrics = accuracy_score(y_test, predictions_bi) + precision_score(y_test, predictions_bi, zero_division=\"warn\") + recall_score(y_test, predictions_bi, zero_division=\"warn\") + f1_score(y_test, predictions_bi, zero_division=\"warn\")\n",
    "        if(bimetrics>bimax):\n",
    "            bimax = bimetrics\n",
    "            bibestC = c\n",
    "            bibestPen = p\n",
    "        print(\"Bigram model\")\n",
    "        print(\"Accuracy score: \", accuracy_score(y_test, predictions_bi))\n",
    "        print(\"Precision score: \", precision_score(y_test, predictions_bi, zero_division=\"warn\"))\n",
    "        print(\"Recall score:\" , recall_score(y_test, predictions_bi, zero_division=\"warn\"))\n",
    "        print(\"F1 score:\" , f1_score(y_test, predictions_bi, zero_division=\"warn\"))\n",
    "        print(\"Confusion Matrix:\", confusion_matrix(y_test, predictions_bi))\n",
    "        '''if(c==10 and p=='l2'):\n",
    "            print(\"---------------\")\n",
    "            print(set(y_test) - set(predictions_uni))\n",
    "            print(set(y_test) - set(predictions_bi))\n",
    "            print(\"---------------\")'''\n",
    "        \n",
    "print(\"BEST UNIGRAM has C=\",unibestC,\" and penalty=\",unibestPen)\n",
    "print(\"BEST BIGRAM has C=\",bibestC,\" and penalty=\",bibestPen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
