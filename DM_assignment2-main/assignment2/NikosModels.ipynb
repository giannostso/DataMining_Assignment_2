{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We stayed at the Schicago Hilton for 4 days an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My $200 Gucci sunglasses were stolen out of my...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hotel is located 1/2 mile from the train stati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Fake\n",
       "0  We stayed at the Schicago Hilton for 4 days an...     1\n",
       "1  My $200 Gucci sunglasses were stolen out of my...     0\n",
       "2  Hotel is located 1/2 mile from the train stati...     1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the 5 csv files, each one containts 160 reviews - 80 Real with label 0 and 80 false with label 1\n",
    "fold1 = pd.read_csv(\"fold1.csv\")\n",
    "fold2 = pd.read_csv(\"fold2.csv\")\n",
    "fold3 = pd.read_csv(\"fold3.csv\")\n",
    "fold4 = pd.read_csv(\"fold4.csv\")\n",
    "fold5 = pd.read_csv(\"fold5.csv\")\n",
    "\n",
    "fold1.head(3)\n",
    "\n",
    "#create a combined dataset for analysis\n",
    "train_df = pd.concat([fold1,fold2,fold3,fold4], ignore_index=True)\n",
    "whole_df = pd.concat([fold1,fold2,fold3,fold4,fold5], ignore_index=True)\n",
    "whole_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is lowercasing, as it is a very common technique in nlp. It converts \"word\", \"WORD\", \"Word\", e.t.c into \"word\" thus making the dictionary of words smaller and also giving the word the weight that it suits it...\n",
    "BUT in many cases some negative reviews have fully capitalized words that shows frustration and anger, for example \"THIS IS HORRIBLE\" and by lowercasing every word we might lose information.. \n",
    "So A custom function suits us that will lowercase the words that start with a Capital character and the rest of the characters are lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>james chicago; the luxurious nice hotel as it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>me, my sister and my best friend all went to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>DO NOT STAY HERE!! my wife and i were visitin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>i stayed in a deluxe king suite. no desk to w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>a recent stay at the james hotel-Chicago, rev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>this hotel does not belong in the same league...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>the hotel was not one of the better ones i st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>i haven't actually stayed at this hotel- yet-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>i had a business trip coming up in chicago an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>this hotel was not worth it. from the moment ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  Fake\n",
       "50   james chicago; the luxurious nice hotel as it...     1\n",
       "51   me, my sister and my best friend all went to ...     0\n",
       "52   DO NOT STAY HERE!! my wife and i were visitin...     1\n",
       "53   i stayed in a deluxe king suite. no desk to w...     0\n",
       "54   a recent stay at the james hotel-Chicago, rev...     1\n",
       "55   this hotel does not belong in the same league...     0\n",
       "56   the hotel was not one of the better ones i st...     1\n",
       "57   i haven't actually stayed at this hotel- yet-...     0\n",
       "58   i had a business trip coming up in chicago an...     1\n",
       "59   this hotel was not worth it. from the moment ...     0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_lowercase(text):\n",
    "    \"\"\"custom function to lowercase the words that only their first character is capital\"\"\"\n",
    "    new_text = \"\"\n",
    "    for word in str(text).split():\n",
    "        if len(word)>1:\n",
    "            if word[0].isupper() and word[1].islower():\n",
    "                word = word[0].lower() + word[1:]\n",
    "                new_text = new_text +\" \"+ word\n",
    "            else:\n",
    "                new_text = new_text +\" \"+ word\n",
    "        elif word[0].isupper():\n",
    "                word = word[0].lower() \n",
    "                new_text = new_text +\" \"+ word\n",
    "        else:\n",
    "            new_text = new_text +\" \"+ word\n",
    "    return new_text\n",
    "\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: custom_lowercase(text))\n",
    "whole_df.iloc[50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing, contractions affect the quality of our Review and some words could be accounted multiple times instead of the correct number. For example \"Y'all\" should be two tokens when examining the review, correct form is \"You all\". \n",
    "So we will transform contractions into their original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>james chicago; the luxurious nice hotel as it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>me, my sister and my best friend all went to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>DO NOT STAY HERE!! my wife and i were visitin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>i stayed in a deluxe king suite. no desk to w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>a recent stay at the james hotel-Chicago, rev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>this hotel does not belong in the same league...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>the hotel was not one of the better ones i st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>i have not actually stayed at this hotel- yet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>i had a business trip coming up in chicago an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>this hotel was not worth it. from the moment ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  Fake\n",
       "50   james chicago; the luxurious nice hotel as it...     1\n",
       "51   me, my sister and my best friend all went to ...     0\n",
       "52   DO NOT STAY HERE!! my wife and i were visitin...     1\n",
       "53   i stayed in a deluxe king suite. no desk to w...     0\n",
       "54   a recent stay at the james hotel-Chicago, rev...     1\n",
       "55   this hotel does not belong in the same league...     0\n",
       "56   the hotel was not one of the better ones i st...     1\n",
       "57   i have not actually stayed at this hotel- yet...     0\n",
       "58   i had a business trip coming up in chicago an...     1\n",
       "59   this hotel was not worth it. from the moment ...     0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contractions\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda x: contractions.fix(x))\n",
    "whole_df.iloc[50:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to remove punctuations from text, we are going to use the already made list from string BUT we are not going to remove \"!\" because in many reviews the number of \"!\" next to words can carry some information. Similar idea to the capitalization..\n",
    "In general we remove puncuation for the same reason as before, to treat \"word\" and \"word.\" as the same, it will lower the number instances in the dictionary and will help assign the proper weights to the \"word\" which has the same meaning in both cases.\n",
    "\n",
    "*IMPORTANT TO STATE: by examining our dataset no emoticons like \":)\" were used in the reviews, so there is no loss of information by removing every puncuation. In case there were emoticons the proper way was to first \"translate\" them with their meaning and then removing all the reaming  punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>james chicago the luxurious nice hotel as it ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>me my sister and my best friend all went to s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>DO NOT STAY HERE!! my wife and i were visitin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>i stayed in a deluxe king suite no desk to wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>a recent stay at the james hotelChicago revea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  Fake\n",
       "50   james chicago the luxurious nice hotel as it ...     1\n",
       "51   me my sister and my best friend all went to s...     0\n",
       "52   DO NOT STAY HERE!! my wife and i were visitin...     1\n",
       "53   i stayed in a deluxe king suite no desk to wo...     0\n",
       "54   a recent stay at the james hotelChicago revea...     1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the \"0\" index of the list is \"!\" so we skip it\n",
    "punctuation_list = string.punctuation[1:]\n",
    "print(punctuation_list)\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove all punctuation except !\"\"\"  \n",
    "    return text.translate(str.maketrans('', '', punctuation_list))\n",
    "\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: remove_punctuation(text))\n",
    "whole_df.iloc[50:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is the removal of stop words from our Reviews. Stopwords usually do not carry any meaning and account for a large number of words, by removing them we have less instances in our dictionary and the training of our algorithms will take less time as less words will be examined each time. \n",
    "NLTK has a list of stopwords which is the common list used in most nlp tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>james chicago luxurious nice hotel advertised ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>sister best friend went stay summer 2004they s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>DO NOT STAY HERE!! wife visiting family chicag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>stayed deluxe king suite desk work sit eat roo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>recent stay james hotelChicago revealed recent...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  Fake\n",
       "50  james chicago luxurious nice hotel advertised ...     1\n",
       "51  sister best friend went stay summer 2004they s...     0\n",
       "52  DO NOT STAY HERE!! wife visiting family chicag...     1\n",
       "53  stayed deluxe king suite desk work sit eat roo...     0\n",
       "54  recent stay james hotelChicago revealed recent...     1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove  stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: remove_stopwords(text))\n",
    "whole_df.iloc[50:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try stemming as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>jame chicago luxuri nice hotel advertis want s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>sister best friend went stay summer 2004they s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>DO not stay here!! wife visit famili chicago d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>stay delux king suit desk work sit eat room se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>recent stay jame hotelchicago reveal recent up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  Fake\n",
       "50  jame chicago luxuri nice hotel advertis want s...     1\n",
       "51  sister best friend went stay summer 2004they s...     0\n",
       "52  DO not stay here!! wife visit famili chicago d...     1\n",
       "53  stay delux king suit desk work sit eat room se...     0\n",
       "54  recent stay jame hotelchicago reveal recent up...     1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: stem_words(text))\n",
    "whole_df.iloc[50:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEMMATIZATION\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to make our dictionary even smaller and the iteration of the algorithms even faster we want to remove frequent used words that are frequent in Both FAKE AND TRUE reviews, because with a balanced dataset such as ours we can't blindly remove frequent words as they could be frequent only at one of the two labels.\n",
    "For that we create a custom function that searches for frequent words in the Reviews of each label seperately, then we compare the list of the 20 most frequent words from each case and remove the words appearing in both cases.\n",
    "\n",
    "*The reaming words from each list will be features with high importance for the label they belong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>james chicago luxurious nice advertise want st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>sister best friend go stay summer 2004they say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>DO NOT STAY HERE!! wife visit family chicago d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>stay deluxe king suite desk work sit eat servi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>recent stay james hotelChicago reveal recent u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  Fake\n",
       "50  james chicago luxurious nice advertise want st...     1\n",
       "51  sister best friend go stay summer 2004they say...     0\n",
       "52  DO NOT STAY HERE!! wife visit family chicago d...     1\n",
       "53  stay deluxe king suite desk work sit eat servi...     0\n",
       "54  recent stay james hotelChicago reveal recent u...     1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#seperate the dataframes\n",
    "#Important to look only on the training set while extracting the frequent words\n",
    "fake_df = train_df.loc[whole_df['Fake'] == 1]\n",
    "real_df = train_df.loc[whole_df['Fake'] == 0]\n",
    "\n",
    "def counts(df):\n",
    "    cnt = Counter()\n",
    "    for text in df[\"Review\"].values:\n",
    "        for word in text.split():\n",
    "            cnt[word] += 1\n",
    "    return cnt\n",
    "\n",
    "fake_cnts = counts(fake_df)\n",
    "real_cnts = counts(real_df)\n",
    "        \n",
    "def get_list(lis):\n",
    "    words = []\n",
    "    for pair in lis:\n",
    "        words.append(pair[0])\n",
    "    return words\n",
    "\n",
    "fake_common = get_list(fake_cnts.most_common(20))\n",
    "real_common = get_list(real_cnts.most_common(20))\n",
    "freq_to_remove = list(set(fake_common).intersection(real_common))\n",
    "\n",
    "def remove_freq(text, freq):\n",
    "    \"\"\"custom function to remove freq words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in freq])\n",
    "\n",
    "whole_df[\"Review\"] = whole_df[\"Review\"].apply(lambda text: remove_freq(text,freq_to_remove))\n",
    "whole_df.iloc[50:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also other pre-processing options. One option is removing Rare words, but rare words could carry meaning and we could also dictate that through the algorithm initialization. \n",
    "Additionally, there were no noise in the data like HTML tags or URLS that should be removed..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we procced to the feature extraction phase, using the TfIdf vectorizer from sklearn so it normalizes the counts of the features that appear as common. Explanation follows from sklearn:\n",
    "With the TFIDFVectorizer the value increases proportionally to count, but is inversely proportional to frequency of the word in the corpus; that is the inverse document frequency (IDF) part.\n",
    "\n",
    "First though we are going to seperate the dataset again into train and test\n",
    "\n",
    "The inverse document frequency adjusts for the fact that some words appear more frequently in general. For example, even though words like “we”, and “the” appear often in documents, since it appears often in all documents, they don’t tell me a lot about what makes this document unique. It’s more interesting to know if words like “crepuscular” or “petrichor” appear a lot in a document because it doesn’t appear frequently in the corpus. “petrichor” tells us a lot more about what the document is about than “we” and “the”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from nltk.tokenize import word_tokenize \n",
    "\n",
    "#whole_df[\"Tokens\"] = whole_df[\"Review\"].apply(lambda text: word_tokenize(text))\n",
    "\n",
    "X = whole_df['Review']\n",
    "Y = whole_df['Fake']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle = False)\n",
    "\n",
    "uni_vector = TfidfVectorizer(lowercase=False, ngram_range=(1, 1))\n",
    "features_uni = uni_vector.fit_transform(X_train).toarray()\n",
    "\n",
    "bi_vector = TfidfVectorizer(lowercase=False, ngram_range=(1, 2))\n",
    "features_bi = bi_vector.fit_transform(X_train).toarray()\n",
    "\n",
    "X_train_trans_uni = uni_vector.fit_transform(X_train)\n",
    "X_test_trans_uni = uni_vector.transform(X_test)\n",
    "\n",
    "X_train_trans_bi = bi_vector.fit_transform(X_train)\n",
    "X_test_trans_bi = bi_vector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets find the 5 most important features for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#features_uni for features per instance\n",
    "names = uni_vector.get_feature_names()\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=5):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=5):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids]#.toarray()\n",
    "    else:\n",
    "        D = Xtr#.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=5):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs\n",
    "\n",
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    ''' Plot the data frames returned by the function plot_tfidf_classfeats(). '''\n",
    "    fig = plt.figure(figsize=(12, 9), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "        ax.set_title(\"label = \" + str(df.label), fontsize=16)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.tfidf, align='center', color='#3F5D7D')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.feature)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.show()\n",
    "    \n",
    "plot_tfidf_classfeats_h(top_feats_by_class(features_uni,y_train, names ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes for both unigram and bigram.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes_uni = MultinomialNB().fit(X_train_trans_uni, y_train)\n",
    "naive_bayes_bi = MultinomialNB().fit(X_train_trans_bi, y_train)\n",
    "\n",
    "#uni\n",
    "predictions_uni = naive_bayes_uni.predict(X_test_trans_uni)\n",
    "\n",
    "#bi\n",
    "predictions_bi = naive_bayes_bi.predict(X_test_trans_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for unigram model\n",
      "Accuracy score:  0.79375\n",
      "Precision score:  0.7238095238095238\n",
      "Recall score: 0.95\n",
      "f1 score: 0.8216216216216217\n",
      "Confusion Matrix: [[51 29]\n",
      " [ 4 76]]\n",
      "for bigram model\n",
      "Accuracy score:  0.83125\n",
      "Precision score:  0.7623762376237624\n",
      "Recall score: 0.9625\n",
      "f1 score: 0.8508287292817679\n",
      "Confusion Matrix: [[56 24]\n",
      " [ 3 77]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "print(\"for unigram model\")\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, predictions_uni))\n",
    "print(\"Precision score: \", precision_score(y_test, predictions_uni))\n",
    "print(\"Recall score:\" , recall_score(y_test, predictions_uni))\n",
    "print(\"f1 score:\" , f1_score(y_test, predictions_uni))\n",
    "print(\"Confusion Matrix:\", confusion_matrix(y_test, predictions_uni))\n",
    "\n",
    "print(\"for bigram model\")\n",
    "print(\"Accuracy score: \", accuracy_score(y_test, predictions_bi))\n",
    "print(\"Precision score: \", precision_score(y_test, predictions_bi))\n",
    "print(\"Recall score:\" , recall_score(y_test, predictions_bi))\n",
    "print(\"f1 score:\" , f1_score(y_test, predictions_bi))\n",
    "print(\"Confusion Matrix:\", confusion_matrix(y_test, predictions_bi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use chi2 to select the 1000 most important features to see if we can improve with a less sparse Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 6325)\n",
      "(640, 46336)\n",
      "\n",
      " trial for k value on unigram model\n",
      "for unigram model with 100 most important features\n",
      "Accuracy score:  0.8125\n",
      "Precision score:  0.7976190476190477\n",
      "Recall score: 0.8375\n",
      "f1 score: 0.8170731707317073\n",
      "Confusion Matrix: [[63 17]\n",
      " [13 67]]\n",
      "-------\n",
      "for unigram model with 300 most important features\n",
      "Accuracy score:  0.8375\n",
      "Precision score:  0.8214285714285714\n",
      "Recall score: 0.8625\n",
      "f1 score: 0.8414634146341463\n",
      "Confusion Matrix: [[65 15]\n",
      " [11 69]]\n",
      "-------\n",
      "for unigram model with 500 most important features\n",
      "Accuracy score:  0.8375\n",
      "Precision score:  0.8068181818181818\n",
      "Recall score: 0.8875\n",
      "f1 score: 0.8452380952380951\n",
      "Confusion Matrix: [[63 17]\n",
      " [ 9 71]]\n",
      "-------\n",
      "for unigram model with 800 most important features\n",
      "Accuracy score:  0.8375\n",
      "Precision score:  0.8\n",
      "Recall score: 0.9\n",
      "f1 score: 0.8470588235294118\n",
      "Confusion Matrix: [[62 18]\n",
      " [ 8 72]]\n",
      "-------\n",
      "for unigram model with 1000 most important features\n",
      "Accuracy score:  0.81875\n",
      "Precision score:  0.7741935483870968\n",
      "Recall score: 0.9\n",
      "f1 score: 0.8323699421965317\n",
      "Confusion Matrix: [[59 21]\n",
      " [ 8 72]]\n",
      "-------\n",
      "for unigram model with 3000 most important features\n",
      "Accuracy score:  0.79375\n",
      "Precision score:  0.7326732673267327\n",
      "Recall score: 0.925\n",
      "f1 score: 0.8176795580110497\n",
      "Confusion Matrix: [[53 27]\n",
      " [ 6 74]]\n",
      "-------\n",
      "best k for unigram model is:  300 , with accuracy:  0.8375\n",
      "\n",
      " trial for k value on bigram model\n",
      "for bigram model with 500 most important feature\n",
      "Accuracy score:  0.83125\n",
      "Precision score:  0.8192771084337349\n",
      "Recall score: 0.85\n",
      "f1 score: 0.8343558282208589\n",
      "Confusion Matrix: [[65 15]\n",
      " [12 68]]\n",
      "-------\n",
      "for bigram model with 1000 most important feature\n",
      "Accuracy score:  0.8375\n",
      "Precision score:  0.8214285714285714\n",
      "Recall score: 0.8625\n",
      "f1 score: 0.8414634146341463\n",
      "Confusion Matrix: [[65 15]\n",
      " [11 69]]\n",
      "-------\n",
      "for bigram model with 1400 most important feature\n",
      "Accuracy score:  0.8375\n",
      "Precision score:  0.8292682926829268\n",
      "Recall score: 0.85\n",
      "f1 score: 0.8395061728395061\n",
      "Confusion Matrix: [[66 14]\n",
      " [12 68]]\n",
      "-------\n",
      "for bigram model with 3000 most important feature\n",
      "Accuracy score:  0.85625\n",
      "Precision score:  0.8131868131868132\n",
      "Recall score: 0.925\n",
      "f1 score: 0.8654970760233918\n",
      "Confusion Matrix: [[63 17]\n",
      " [ 6 74]]\n",
      "-------\n",
      "for bigram model with 5000 most important feature\n",
      "Accuracy score:  0.825\n",
      "Precision score:  0.776595744680851\n",
      "Recall score: 0.9125\n",
      "f1 score: 0.8390804597701149\n",
      "Confusion Matrix: [[59 21]\n",
      " [ 7 73]]\n",
      "-------\n",
      "for bigram model with 10000 most important feature\n",
      "Accuracy score:  0.825\n",
      "Precision score:  0.7708333333333334\n",
      "Recall score: 0.925\n",
      "f1 score: 0.840909090909091\n",
      "Confusion Matrix: [[58 22]\n",
      " [ 6 74]]\n",
      "-------\n",
      "best k for unigram model is:  3000 , with accuracy:  0.85625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "#initial features\n",
    "print(features_uni.shape)\n",
    "print(features_bi.shape)\n",
    "\n",
    "#try diffent values for k for both models\n",
    "k_uni = [100,300,500,800,1000,3000]\n",
    "k_bi = [500,1000,1400,3000,5000,10000]\n",
    "print(\"\\n trial for k value on unigram model\")\n",
    "best_acc = 0 \n",
    "best = 0\n",
    "for trial in k_uni:\n",
    "    chi2_features_uni = SelectKBest(chi2, k = trial) \n",
    "    X_best_features_uni = chi2_features_uni.fit_transform(X_train_trans_uni, y_train) \n",
    "    X_best_features_uni_test = chi2_features_uni.transform(X_test_trans_uni)\n",
    "\n",
    "    naive_bayes_uni = MultinomialNB().fit(X_best_features_uni, y_train)\n",
    "    predictions_uni = naive_bayes_uni.predict(X_best_features_uni_test)\n",
    "    print(\"for unigram model with %d most important features\" %(trial))\n",
    "    acc = accuracy_score(y_test, predictions_uni)\n",
    "    print(\"Accuracy score: \", acc)\n",
    "    print(\"Precision score: \", precision_score(y_test, predictions_uni))\n",
    "    print(\"Recall score:\" , recall_score(y_test, predictions_uni))\n",
    "    print(\"f1 score:\" , f1_score(y_test, predictions_uni))\n",
    "    print(\"Confusion Matrix:\", confusion_matrix(y_test, predictions_uni))\n",
    "    print(\"-------\")\n",
    "    if acc> best_acc:\n",
    "        best_acc = acc\n",
    "        best = trial\n",
    "print(\"best k for unigram model is: \",best,\", with accuracy: \",best_acc )    \n",
    "    \n",
    "print(\"\\n trial for k value on bigram model\") \n",
    "best_acc = 0 \n",
    "best = 0\n",
    "for trial in k_bi:\n",
    "    chi2_features_bi = SelectKBest(chi2, k = trial) \n",
    "    X_best_features_bi = chi2_features_bi.fit_transform(X_train_trans_bi, y_train) \n",
    "    X_best_features_bi_test = chi2_features_bi.transform(X_test_trans_bi)\n",
    "    \n",
    "    naive_bayes_bi = MultinomialNB().fit(X_best_features_bi, y_train)\n",
    "    predictions_bi = naive_bayes_bi.predict(X_best_features_bi_test)\n",
    "    print(\"for bigram model with %d most important feature\" %(trial))\n",
    "    acc = accuracy_score(y_test, predictions_bi)\n",
    "    print(\"Accuracy score: \", acc)\n",
    "    print(\"Precision score: \", precision_score(y_test, predictions_bi))\n",
    "    print(\"Recall score:\" , recall_score(y_test, predictions_bi))\n",
    "    print(\"f1 score:\" , f1_score(y_test, predictions_bi))\n",
    "    print(\"Confusion Matrix:\", confusion_matrix(y_test, predictions_bi))\n",
    "    print(\"-------\")\n",
    "    if acc> best_acc:\n",
    "        best_acc = acc\n",
    "        best = trial\n",
    "print(\"best k for unigram model is: \",best,\", with accuracy: \",best_acc )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
